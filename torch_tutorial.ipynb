{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik9M2igT0XWf"
   },
   "outputs": [],
   "source": [
    "import collections, time, math, random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWus4HvyKtos"
   },
   "source": [
    "# Writing a simple model in PyTorch\n",
    "\n",
    "This notebook shows you how to get started with PyTorch and also provides you some skeleton code. You can make a copy of the notebook and write your solution in it, or you can download it (**File &rarr; Download .py**) and work on it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S13j4P_ZLjK5"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Clone the HW1 repository. (If you rerun the notebook, you'll get an error that directory `hw1` already exists, which you can ignore.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICSyqjzMLk2C",
    "outputId": "6b93bc4c-b710-4c3b-e2f9-0a3f7826c58b"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ND-CSE-40657/hw1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-6Ff7j9Qu_v"
   },
   "source": [
    "Import PyTorch. If you want to run on your own computer, you'll need to install PyTorch, which is usually as simple as `pip install torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1gqmSDrQ6Nc",
    "outputId": "605d1d06-f98f-49c2-f09f-8d23ce09a400"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'Using Torch v{torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bNQMVSzTQu0"
   },
   "source": [
    "Check for a GPU. A GPU is not necessary for this assignment -- in fact, for the size of model we're training, it probably makes things slower. To enable/disable GPU, go to **Runtime &rarr; Change runtime type &rarr; Hardware accelerator** and select **GPU** (to enable the GPU) or **None** (to disable the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhOxkWlJQu5Q",
    "outputId": "5efb6326-474e-450b-de9b-f93fdb5d919b"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 0:\n",
    "    print(f'Using GPU ({torch.cuda.get_device_name(0)})')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('Using CPU')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5s02IiCdSbno"
   },
   "source": [
    "## Read and preprocess data\n",
    "\n",
    "Read in the data files. Note that we strip trailing newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJipdajNwBZ-"
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    return [list(line.rstrip('\\n')) + ['<EOS>'] for line in open(filename)]\n",
    "traindata = read_data('hw1/data/train')\n",
    "devdata = read_data('hw1/data/dev')\n",
    "testdata = read_data('hw1/data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiioHKwZxFqe"
   },
   "source": [
    "Create a vocabulary containing the most frequent words and some special words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bmt1wc9zwTNN"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counts, size):\n",
    "        if not isinstance(counts, collections.Counter):\n",
    "            raise TypeError('counts must be a collections.Counter')\n",
    "        words = {'<EOS>', '<UNK>'}\n",
    "        for word, _ in counts.most_common():\n",
    "            words.add(word)\n",
    "            if len(words) == size:\n",
    "                break\n",
    "        self.num_to_word = list(words)    \n",
    "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.num_to_word)\n",
    "    def __iter__(self):\n",
    "        return iter(self.num_to_word)\n",
    "\n",
    "    def numberize(self, word):\n",
    "        if word in self.word_to_num:\n",
    "            return self.word_to_num[word]\n",
    "        else: \n",
    "            return self.word_to_num['<UNK>']\n",
    "\n",
    "    def denumberize(self, num):\n",
    "        return self.num_to_word[num]\n",
    "\n",
    "chars = collections.Counter()\n",
    "for line in traindata:\n",
    "    chars.update(line)\n",
    "vocab = Vocab(chars, 100) # For our data, 100 is a good size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-CBxHYz1pKO"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "Now we want to define a unigram language model. The parameters of the model are _logits_ $\\mathbf{s}$, which are unconstrained real numbers, and we will apply a softmax to change them into probabilities (which are nonnegative and sum to one).\n",
    "\n",
    "\\begin{align}\n",
    "P(i) &= [\\operatorname{softmax} \\mathbf{s}]_i \\\\\n",
    "&= \\frac{\\exp s_i}{\\sum_{i'} \\exp s_{i'}}.\n",
    "\\end{align}\n",
    "\n",
    "Create an array (a `Tensor`) of logits, one for each word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjxfIW1QR5T1"
   },
   "outputs": [],
   "source": [
    "logits = torch.normal(mean=0, std=0.01, \n",
    "                      size=(len(vocab),), \n",
    "                      requires_grad=True, \n",
    "                      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSrCsbknSKCu"
   },
   "source": [
    "The function `torch.normal` creates an array of random numbers, normally distributed (here with mean zero and standard deviation 0.01).\n",
    "\n",
    "The `size` argument says that it should be a one-dimensional array with `vocab.size` elements, one for each word in the vocabulary.\n",
    "\n",
    "The next two arguments are important. The `requires_grad` argument tells PyTorch that we will want to compute gradients with respect to `logits`, because we want to learn its values. The `device` argument says where to store the array.\n",
    "\n",
    "There are a couple of functions below that will want to know what the parameters of our model are. So we make a list for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EWMvMbyTuvu"
   },
   "outputs": [],
   "source": [
    "parameters = [logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw3lcnd3YVqp"
   },
   "source": [
    "Next, we write code to convert the logits into probabilities -- actually, log-probabilities. Torch has a function that does a softmax and a log together; it's more numerically stable than doing them in two steps. (Even though `logits` has only one dimension, we still have to say `dim=0` to specify which dimension the softmax should be computed over.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HTxqafBXC-O"
   },
   "outputs": [],
   "source": [
    "def logprobs():\n",
    "    return torch.log_softmax(logits, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1JbNrXbhmEm"
   },
   "source": [
    "This returns an array of floats like you'd expect, but this array also remembers _how_ it was computed. PyTorch will use this information to compute gradients for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qM0abXf41r9q"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Next, we create an optimizer, whose job is to adjust a set of parameters to minimize a loss function. Here, we're using `SGD` (stochastic gradient descent); other options are `Adagrad`, `Adam`, and others. Different optimizers take different options. Here, `lr` stands for \"learning rate\" and we usually try different powers of ten until we get the best results on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOg9hxbYJepL"
   },
   "outputs": [],
   "source": [
    "o = torch.optim.SGD(parameters, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8MPOo1OJpUv"
   },
   "source": [
    "Next, we run through the training data a few times (epochs). For each sentence, move the parameters a little bit to decrease the loss function. If you want to rerun the training, go to **Run &rarr; Restart and run all** or **Runtime &rarr; Run all**. It takes about 5 minutes per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w050ph5ivows",
    "outputId": "3150869e-d092-4ea2-a9c4-272d48f9f790"
   },
   "outputs": [],
   "source": [
    "prev_dev_acc = None\n",
    "\n",
    "for epoch in range(100):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Run through the training data\n",
    "\n",
    "    random.shuffle(traindata) # Important\n",
    "\n",
    "    train_loss = 0  # Total negative log-probability\n",
    "    train_chars = 0 # Total number of characters\n",
    "    for chars in traindata:\n",
    "        # Compute the negative log-likelihood of this line,\n",
    "        # which is the thing we want to minimize.\n",
    "        loss = 0.\n",
    "        for c in chars:\n",
    "            train_chars += 1\n",
    "            loss -= logprobs()[vocab.numberize(c)]\n",
    "\n",
    "        # Keep a running total of negative log-likelihood.\n",
    "        # The .item() turns a one-element tensor into an ordinary float,\n",
    "        # including detaching the history of how it was computed,\n",
    "        # so we don't save the history across sentences.\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute gradient of loss with respect to parameters.\n",
    "        o.zero_grad()   # Reset gradients to zero\n",
    "        loss.backward() # Add in the gradient of loss\n",
    "\n",
    "        # Clip gradients (not needed here, but helpful for RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(parameters, 1.0)\n",
    "\n",
    "        # Do one step of gradient descent.\n",
    "        o.step()\n",
    "\n",
    "    # Run through the development data\n",
    "\n",
    "    dev_chars = 0   # Total number of characters\n",
    "    dev_correct = 0 # Total number of characters guessed correctly\n",
    "    for chars in devdata:\n",
    "        for c in chars:\n",
    "            dev_chars += 1\n",
    "\n",
    "            # Find the character with highest predicted probability.\n",
    "            # The .item() is needed to change a one-element tensor to\n",
    "            # an ordinary int.\n",
    "            best = vocab.denumberize(logprobs().argmax().item())\n",
    "            if best == c:\n",
    "                dev_correct += 1\n",
    "\n",
    "    dev_acc = dev_correct/dev_chars\n",
    "    print(f'time={time.time()-epoch_start} train_ppl={math.exp(train_loss/train_chars)} dev_acc={dev_acc}')\n",
    "\n",
    "    # Important: If dev accuracy didn't improve, halve the learning rate\n",
    "    if prev_dev_acc is not None and dev_acc <= prev_dev_acc:\n",
    "            o.param_groups[0]['lr'] *= 0.5\n",
    "            print(f\"lr={o.param_groups[0]['lr']}\")\n",
    "\n",
    "    # When the learning rate gets too low, stop training\n",
    "    if o.param_groups[0]['lr'] < 0.01:\n",
    "        break\n",
    "\n",
    "    prev_dev_acc = dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJCDl1wXcE1"
   },
   "source": [
    "## Matrix multiplication\n",
    "\n",
    "Not illustrated above is matrix multiplication using the [`@` operator](https://pytorch.org/docs/stable/generated/torch.matmul.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iT09tYHoYVxL",
    "outputId": "60e13071-74c7-4af8-d1f3-10b3b5fe7e3a"
   },
   "outputs": [],
   "source": [
    "A = torch.ones(2,3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FO8MdizqYt06",
    "outputId": "24bb8d25-1ee1-4226-fb52-a89e7670329a"
   },
   "outputs": [],
   "source": [
    "b = torch.ones(3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0dasSgiYwku",
    "outputId": "8a3244e3-e255-439c-ff89-fed37bc8d983"
   },
   "outputs": [],
   "source": [
    "C = torch.ones(3,5)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z9oP2T5YmH1"
   },
   "source": [
    "Matrix-vector product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tkp9ONLYiXO",
    "outputId": "0dc85580-589f-4db2-f7ed-8dbef145659d"
   },
   "outputs": [],
   "source": [
    "A @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCx8OmZFYoPD"
   },
   "source": [
    "Vector-matrix product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5H2O3CFYpwL",
    "outputId": "5e2ee1d1-543b-4a8c-c8ea-fabde8232210"
   },
   "outputs": [],
   "source": [
    "b @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAwFWeyFYx5J"
   },
   "source": [
    "Matrix-matrix product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yA-BPRD3YzL_",
    "outputId": "62c122ee-7b81-4cb1-a3e6-0119cfb373aa"
   },
   "outputs": [],
   "source": [
    "A @ C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZgmnHc8ZpyV"
   },
   "source": [
    "The `@` operator works even if its arguments have more than two dimensions, but the semantics can be a little bit confusing. It's probably nicer to use [`einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) instead. For example, suppose we needed to compute\n",
    "\n",
    "$$B_{ba} = \\sum_{m=1}^5 \\sum_{n=1}^4 l_{bn} A_{anm} r_{bm} \\qquad (a=1,2,3; b=1,2).$$\n",
    "\n",
    "We can do this using a single call to einsum, without looping over $a,b,m$, or $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9s70asFZgeSr",
    "outputId": "d74a5820-4523-42e2-dd7e-90a1c61e6866"
   },
   "outputs": [],
   "source": [
    "A = torch.ones(3,5,4)\n",
    "l = torch.ones(2,5)\n",
    "r = torch.ones(2,4)\n",
    "B = torch.einsum('bn,anm,bm->ba', l, A, r)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDJkw3kKgrNR"
   },
   "source": [
    "The arguments `l`, `A`, and `r` are the three tensors being combined, and `B` is the result tensor. The first argument is the instructions for how to do the combination. Each letter acts like an index variable, and internally `einsum` loops over all of them. The `bn` are the indices for `l`, the `anm` are the indices for `A`, the `bm` are the indices for `r`, and the `ba` are the indices for `B`. The computation is always a sum of products; it's equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SZlkc9SjmxO",
    "outputId": "7b22e95e-cde6-4c1d-b735-9b62cfdf7ac3"
   },
   "outputs": [],
   "source": [
    "B = torch.zeros(2,3)\n",
    "for a in range(3):\n",
    "    for b in range(2):\n",
    "        for m in range(4):\n",
    "           for n in range(5):\n",
    "               B[b,a] += l[b,n] * A[a,n,m] * r[b,m]\n",
    "B               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWTKoVTwad6s"
   },
   "source": [
    "## Saving and loading\n",
    "\n",
    "You may want to save a model to disk so you can continue training it later or use it later. We save the vocabulary as well so that we preserve the mapping from characters to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdttQjHRdRXY"
   },
   "outputs": [],
   "source": [
    "torch.save((parameters, vocab), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmdBXCvldVyX"
   },
   "outputs": [],
   "source": [
    "(parameters_copy, vocab_copy) = torch.load('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoMsIjyqdfWn"
   },
   "source": [
    "In Colab, however, the saved models won't persist across sessions. See the [Colab docs](https://colab.research.google.com/notebooks/io.ipynb) for some options for persistent storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "\n",
    "Torch provides a class `torch.nn.Module` that helps you to manage all the parameters of your model, and other associated information, as a single object. However, it does some magic that I find slightly confusing, so I chose not to use it at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram(torch.nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        # Call parent class's __init__(). You will get an error\n",
    "        # if you forget this.\n",
    "        super().__init__() \n",
    "        \n",
    "        # Store the vocab inside the Unigram object,\n",
    "        # so when we save the Unigram, it saves the vocab too.\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Create the model parameters. Wrapping it inside\n",
    "        # Parameter tells Module to manage it for us, so\n",
    "        # we don't have to set requires_grad or device.\n",
    "        self.logits = torch.nn.Parameter(\n",
    "                        torch.normal(mean=0, std=0.01, \n",
    "                                     size=(len(vocab),))\n",
    "                      )\n",
    "    \n",
    "    # You can define whatever methods you want,\n",
    "    # but \"forward\" is special, so the main method should\n",
    "    # be named \"forward\".\n",
    "    def forward(self):\n",
    "        return torch.log_softmax(self.logits, dim=0)\n",
    "    \n",
    "model = Unigram(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whatever we do with `model` gets done to everything inside it. A Module can even contain other Modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move all of model's parameters to device\n",
    "model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer for all of model's parameters\n",
    "o = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/load model to/from disk\n",
    "torch.save(model, 'model')\n",
    "model = torch.load('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` object can be called like a function, which invokes `model.forward()`. But `model()` also calls various _hooks_ so it's recommended that you always write `model()`, not `model.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

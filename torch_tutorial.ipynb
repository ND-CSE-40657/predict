{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch Tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik9M2igT0XWf"
      },
      "source": [
        "import collections, time, math, random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWus4HvyKtos"
      },
      "source": [
        "# Writing a simple model in PyTorch\n",
        "\n",
        "This notebook shows you how to get started with PyTorch and also provides you some skeleton code. You can make a copy of the notebook and write your solution in it, or you can download it (**File &rarr; Download .py**) and work on it locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S13j4P_ZLjK5"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Clone the HW1 repository. (If you rerun the notebook, you'll get an error that directory `hw1` already exists, which you can ignore.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICSyqjzMLk2C",
        "outputId": "6b93bc4c-b710-4c3b-e2f9-0a3f7826c58b"
      },
      "source": [
        "!git clone https://github.com/ND-CSE-40657/hw1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw1'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 97 (delta 26), reused 43 (delta 13), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (97/97), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-6Ff7j9Qu_v"
      },
      "source": [
        "Import PyTorch. If you want to run on your own computer, you'll need to install PyTorch, which is usually as simple as `pip install torch`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1gqmSDrQ6Nc",
        "outputId": "605d1d06-f98f-49c2-f09f-8d23ce09a400"
      },
      "source": [
        "import torch\n",
        "print(f'Using Torch v{torch.__version__}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Torch v1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bNQMVSzTQu0"
      },
      "source": [
        "Check for a GPU. A GPU is not necessary for this assignment -- in fact, for the size of model we're training, it probably makes things slower. To enable/disable GPU, go to **Runtime &rarr; Change runtime type &rarr; Hardware accelerator** and select **GPU** (to enable the GPU) or **None** (to disable the GPU)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhOxkWlJQu5Q",
        "outputId": "5efb6326-474e-450b-de9b-f93fdb5d919b"
      },
      "source": [
        "if torch.cuda.device_count() > 0:\n",
        "    print(f'Using GPU ({torch.cuda.get_device_name(0)})')\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print('Using CPU')\n",
        "    device = 'cpu'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s02IiCdSbno"
      },
      "source": [
        "## Read and preprocess data\n",
        "\n",
        "Read in the data files. Note that we strip trailing newlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJipdajNwBZ-"
      },
      "source": [
        "def read_data(filename):\n",
        "    return [list(line.rstrip('\\n')) + ['<EOS>'] for line in open(filename)]\n",
        "traindata = read_data('hw1/data/train')\n",
        "devdata = read_data('hw1/data/dev')\n",
        "testdata = read_data('hw1/data/test')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiioHKwZxFqe"
      },
      "source": [
        "Create a vocabulary containing the most frequent words and some special words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmt1wc9zwTNN"
      },
      "source": [
        "class Vocab:\n",
        "    def __init__(self, counts, size):\n",
        "        if not isinstance(counts, collections.Counter):\n",
        "            raise TypeError('counts must be a collections.Counter')\n",
        "        words = {'<EOS>', '<UNK>'}\n",
        "        for word, _ in counts.most_common():\n",
        "            words.add(word)\n",
        "            if len(words) == size:\n",
        "                break\n",
        "        self.num_to_word = list(words)    \n",
        "        self.word_to_num = {word:num for num, word in enumerate(self.num_to_word)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.num_to_word)\n",
        "    def __iter__(self):\n",
        "        return iter(self.num_to_word)\n",
        "\n",
        "    def numberize(self, word):\n",
        "        if word in self.word_to_num:\n",
        "            return self.word_to_num[word]\n",
        "        else: \n",
        "            return self.word_to_num['<UNK>']\n",
        "\n",
        "    def denumberize(self, num):\n",
        "        return self.num_to_word[num]\n",
        "\n",
        "chars = collections.Counter()\n",
        "for line in traindata:\n",
        "    chars.update(line)\n",
        "vocab = Vocab(chars, 100) # For our data, 100 is a good size."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-CBxHYz1pKO"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Now we want to define a unigram language model. The parameters of the model are _logits_ $\\mathbf{s}$, which are unconstrained real numbers, and we will apply a softmax to change them into probabilities (which are nonnegative and sum to one).\n",
        "\n",
        "\\begin{align}\n",
        "P(i) &= [\\operatorname{softmax} \\mathbf{s}]_i \\\\\n",
        "&= \\frac{\\exp s_i}{\\sum_{i'} \\exp s_{i'}}.\n",
        "\\end{align}\n",
        "\n",
        "Create an array (a `Tensor`) of logits, one for each word in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjxfIW1QR5T1"
      },
      "source": [
        "logits = torch.normal(mean=0, std=0.01, \n",
        "                      size=(len(vocab),), \n",
        "                      requires_grad=True, \n",
        "                      device=device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSrCsbknSKCu"
      },
      "source": [
        "The function `torch.normal` creates an array of random numbers, normally distributed (here with mean zero and standard deviation 0.01).\n",
        "\n",
        "The `size` argument says that it should be a one-dimensional array with `vocab.size` elements, one for each word in the vocabulary.\n",
        "\n",
        "The next two arguments are important. The `requires_grad` argument tells PyTorch that we will want to compute gradients with respect to `logits`, because we want to learn its values. The `device` argument says where to store the array.\n",
        "\n",
        "There are a couple of functions below that will want to know what the parameters of our model are. So we make a list for future use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EWMvMbyTuvu"
      },
      "source": [
        "parameters = [logits]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw3lcnd3YVqp"
      },
      "source": [
        "Next, we write code to convert the logits into probabilities -- actually, log-probabilities. Torch has a function that does a softmax and a log together; it's more numerically stable than doing them in two steps. (Even though `logits` has only one dimension, we still have to say `dim=0` to specify which dimension the softmax should be computed over.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HTxqafBXC-O"
      },
      "source": [
        "def logprobs():\n",
        "    return torch.log_softmax(logits, dim=0)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1JbNrXbhmEm"
      },
      "source": [
        "This returns an array of floats like you'd expect, but this array also remembers _how_ it was computed. PyTorch will use this information to compute gradients for learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM0abXf41r9q"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Next, we create an optimizer, whose job is to adjust a set of parameters to minimize a loss function. Here, we're using `SGD` (stochastic gradient descent); other options are `Adagrad`, `Adam`, and others. Different optimizers take different options. Here, `lr` stands for \"learning rate\" and we usually try different powers of ten until we get the best results on the dev set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOg9hxbYJepL"
      },
      "source": [
        "o = torch.optim.SGD(parameters, lr=0.1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8MPOo1OJpUv"
      },
      "source": [
        "Next, we run through the training data a few times (epochs). For each sentence, move the parameters a little bit to decrease the loss function. If you want to rerun the training, go to **Run &rarr; Restart and run all** or **Runtime &rarr; Run all**. It takes about 5 minutes per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w050ph5ivows",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3150869e-d092-4ea2-a9c4-272d48f9f790"
      },
      "source": [
        "prev_dev_acc = None\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Run through the training data\n",
        "\n",
        "    random.shuffle(traindata) # Important\n",
        "\n",
        "    train_loss = 0  # Total negative log-probability\n",
        "    train_chars = 0 # Total number of characters\n",
        "    for chars in traindata:\n",
        "        # Compute the negative log-likelihood of this line,\n",
        "        # which is the thing we want to minimize.\n",
        "        loss = 0.\n",
        "        for c in chars:\n",
        "            train_chars += 1\n",
        "            loss -= logprobs()[vocab.numberize(c)]\n",
        "\n",
        "        # Keep a running total of negative log-likelihood.\n",
        "        # The .item() turns a one-element tensor into an ordinary float,\n",
        "        # including detaching the history of how it was computed,\n",
        "        # so we don't save the history across sentences.\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Compute gradient of loss with respect to parameters.\n",
        "        o.zero_grad()   # Reset gradients to zero\n",
        "        loss.backward() # Add in the gradient of loss\n",
        "\n",
        "        # Clip gradients (not needed here, but helpful for RNNs)\n",
        "        torch.nn.utils.clip_grad_norm_(parameters, 1.0)\n",
        "\n",
        "        # Do one step of gradient descent.\n",
        "        o.step()\n",
        "\n",
        "    # Run through the development data\n",
        "\n",
        "    dev_chars = 0   # Total number of characters\n",
        "    dev_correct = 0 # Total number of characters guessed correctly\n",
        "    for chars in devdata:\n",
        "        for c in chars:\n",
        "            dev_chars += 1\n",
        "\n",
        "            # Find the character with highest predicted probability.\n",
        "            # The .item() is needed to change a one-element tensor to\n",
        "            # an ordinary int.\n",
        "            best = vocab.denumberize(logprobs().argmax().item())\n",
        "            if best == c:\n",
        "                dev_correct += 1\n",
        "\n",
        "    dev_acc = dev_correct/dev_chars\n",
        "    print(f'time={time.time()-epoch_start} train_ppl={math.exp(train_loss/train_chars)} dev_acc={dev_acc}')\n",
        "\n",
        "    # Important: If dev accuracy didn't improve, halve the learning rate\n",
        "    if prev_dev_acc is not None and dev_acc <= prev_dev_acc:\n",
        "            o.param_groups[0]['lr'] *= 0.5\n",
        "            print(f\"lr={o.param_groups[0]['lr']}\")\n",
        "\n",
        "    # When the learning rate gets too low, stop training\n",
        "    if o.param_groups[0]['lr'] < 0.01:\n",
        "        break\n",
        "\n",
        "    prev_dev_acc = dev_acc"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time=175.40006589889526 train_ppl=27.24637583953369 dev_acc=0.16477499004380725\n",
            "time=174.86245346069336 train_ppl=27.16949725655569 dev_acc=0.16477499004380725\n",
            "lr=0.05\n",
            "time=180.46191382408142 train_ppl=27.11631087489004 dev_acc=0.16477499004380725\n",
            "lr=0.025\n",
            "time=175.17623615264893 train_ppl=27.09093649484456 dev_acc=0.16477499004380725\n",
            "lr=0.0125\n",
            "time=174.8559775352478 train_ppl=27.077736270054164 dev_acc=0.16477499004380725\n",
            "lr=0.00625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsJCDl1wXcE1"
      },
      "source": [
        "## Matrix multiplication\n",
        "\n",
        "Not illustrated above is matrix multiplication using the [`@` operator](https://pytorch.org/docs/stable/generated/torch.matmul.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT09tYHoYVxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e13071-74c7-4af8-d1f3-10b3b5fe7e3a"
      },
      "source": [
        "A = torch.ones(2,3)\n",
        "A"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO8MdizqYt06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24bb8d25-1ee1-4226-fb52-a89e7670329a"
      },
      "source": [
        "b = torch.ones(3)\n",
        "b"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dasSgiYwku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3244e3-e255-439c-ff89-fed37bc8d983"
      },
      "source": [
        "C = torch.ones(3,5)\n",
        "C"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z9oP2T5YmH1"
      },
      "source": [
        "Matrix-vector product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tkp9ONLYiXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc85580-589f-4db2-f7ed-8dbef145659d"
      },
      "source": [
        "A @ b"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCx8OmZFYoPD"
      },
      "source": [
        "Vector-matrix product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5H2O3CFYpwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2ee1d1-543b-4a8c-c8ea-fabde8232210"
      },
      "source": [
        "b @ C"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3., 3., 3., 3., 3.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAwFWeyFYx5J"
      },
      "source": [
        "Matrix-matrix product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA-BPRD3YzL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c122ee-7b81-4cb1-a3e6-0119cfb373aa"
      },
      "source": [
        "A @ C"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3., 3., 3., 3., 3.],\n",
              "        [3., 3., 3., 3., 3.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZgmnHc8ZpyV"
      },
      "source": [
        "The `@` operator works even if its arguments have more than two dimensions, but the semantics can be a little bit confusing. It's probably nicer to use [`einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) instead. For example, suppose we needed to compute\n",
        "\n",
        "$$B_{ba} = \\sum_{m=1}^5 \\sum_{n=1}^4 l_{bn} A_{anm} r_{bm} \\qquad (a=1,2,3; b=1,2).$$\n",
        "\n",
        "We can do this using a single call to einsum, without looping over $a,b,m$, or $n$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s70asFZgeSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74a5820-4523-42e2-dd7e-90a1c61e6866"
      },
      "source": [
        "A = torch.ones(3,5,4)\n",
        "l = torch.ones(2,5)\n",
        "r = torch.ones(2,4)\n",
        "B = torch.einsum('bn,anm,bm->ba', l, A, r)\n",
        "B"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20., 20., 20.],\n",
              "        [20., 20., 20.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDJkw3kKgrNR"
      },
      "source": [
        "The arguments `l`, `A`, and `r` are the three tensors being combined, and `B` is the result tensor. The first argument is the instructions for how to do the combination. Each letter acts like an index variable, and internally `einsum` loops over all of them. The `bn` are the indices for `l`, the `anm` are the indices for `A`, the `bm` are the indices for `r`, and the `ba` are the indices for `B`. The computation is always a sum of products; it's equivalent to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SZlkc9SjmxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b22e95e-cde6-4c1d-b735-9b62cfdf7ac3"
      },
      "source": [
        "B = torch.zeros(2,3)\n",
        "for a in range(3):\n",
        "    for b in range(2):\n",
        "        for m in range(4):\n",
        "           for n in range(5):\n",
        "               B[b,a] += l[b,n] * A[a,n,m] * r[b,m]\n",
        "B               "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20., 20., 20.],\n",
              "        [20., 20., 20.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWTKoVTwad6s"
      },
      "source": [
        "## Saving and loading\n",
        "\n",
        "You may want to save a model to disk so you can continue training it later or use it later. We save the vocabulary as well so that we preserve the mapping from characters to numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdttQjHRdRXY"
      },
      "source": [
        "torch.save((parameters, vocab), 'model')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmdBXCvldVyX"
      },
      "source": [
        "(parameters_copy, vocab_copy) = torch.load('model')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoMsIjyqdfWn"
      },
      "source": [
        "In Colab, however, the saved models won't persist across sessions. See the [Colab docs](https://colab.research.google.com/notebooks/io.ipynb) for some options for persistent storage."
      ]
    }
  ]
}